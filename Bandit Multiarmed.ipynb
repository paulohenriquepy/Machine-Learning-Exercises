{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d49bdb38-482e-4417-9154-8f825b00457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290918cd-bd42-4726-a2a5-a34b8cab03ae",
   "metadata": {},
   "source": [
    "**1. Creating a function that returns a randomly ordered list of probabilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07293bdf-877b-44ef-91c4-6aed184abeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_bandits():\n",
    "    \"\"\"\n",
    "    Function that returns a list of randomly shuffled probabilities.\n",
    "    This list contains the reward probability of each \"slot machine\" (bandit).\n",
    "    \"\"\"\n",
    "    bandits = [0.1, 0.1, 0.1, 0.2, 0.6]\n",
    "    random.shuffle(bandits)\n",
    "    return bandits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dea914-c0ea-46b9-815f-29309e653376",
   "metadata": {},
   "source": [
    "**2. A function that creates the game itself**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2246ebae-57eb-4736-a29a-e0401db619b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_armed_bandit(num_games=1000, epsilon=0.1, verbose=False):\n",
    "    \n",
    "    bandits = gen_bandits()\n",
    "    total_reward = 0\n",
    "    acum_reward_bandit = np.zeros(len(bandits))  # total reward accumulated per image\n",
    "    num_selected_bandit = np.zeros(len(bandits)) # number of times each image was selected\n",
    "    q_bandits = np.zeros(len(bandits))           # estimated value of average reward per image\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Initial Bandits Distribution\\n  {}\".format(bandits))\n",
    "    \n",
    "    for game in range(0, num_games):\n",
    "        \n",
    "        # Save the previous Q(a) values before the update\n",
    "        # Initially, the values are zero\n",
    "        # But after each iteration, the new values are saved\n",
    "        old_q_bandits = q_bandits.copy()\n",
    "        \n",
    "        # Select the \"slot machine\" to play\n",
    "        # Generate a random number between 0 and 1 with uniform distribution\n",
    "        # This value will usually be greater than 0.1\n",
    "        # That will cause the algorithm to prefer exploiting\n",
    "        if np.random.random() < epsilon:\n",
    "            bandit = np.random.randint(len(bandits))  # Explore\n",
    "        else:\n",
    "            # Select a bandit with the highest estimated Q value\n",
    "            # This creates a boolean array where True is in the position(s) of the max Q\n",
    "            # np.flatnonzero returns the indices with True\n",
    "            bandit = np.random.choice(np.flatnonzero(q_bandits == q_bandits.max()))  # Exploit\n",
    "            \n",
    "        # Get the reward\n",
    "        # If a randomly drawn number is less than the reward probability, you get 1 point\n",
    "        reward = 1 if (np.random.random() < bandits[bandit]) else 0\n",
    "        \n",
    "        # Add the reward to the total\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Update the values for the selected \"slot machine\"\n",
    "        acum_reward_bandit[bandit] += reward  # Update the total reward for this bandit\n",
    "        num_selected_bandit[bandit] += 1      # Update the number of times this bandit was selected\n",
    "        q_bandits[bandit] = acum_reward_bandit[bandit] / num_selected_bandit[bandit]  # Update Q(a) as the running average\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\nGAME {game}\\n  Old Q_Bandits = {old_q_bandits}\\n  Selected Bandit = {bandit} \\\n",
    "                  \\n  Reward = {reward}\\n  Q_Bandits = {q_bandits}\"\n",
    "                  .format(game=game+1, old_q_bandits=old_q_bandits, bandit=bandit, \n",
    "                          reward=reward, q_bandits=q_bandits))\n",
    "    \n",
    "    return bandits, total_reward, q_bandits, num_selected_bandit, acum_reward_bandit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7f227f-34c3-41a1-89a8-18c114400210",
   "metadata": {},
   "source": [
    "**3 Trying differente values of exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed2a8629-4f08-4c7e-8742-ccec0dc35db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epsilon': 0.0, 'total_reward': 629, 'imagem_mais_exibida': 4, 'imagem_mais_cliques': 4, 'q_max': np.float64(0.629)}\n",
      "{'epsilon': 0.01, 'total_reward': 610, 'imagem_mais_exibida': 4, 'imagem_mais_cliques': 4, 'q_max': np.float64(0.6108)}\n",
      "{'epsilon': 0.02, 'total_reward': 369, 'imagem_mais_exibida': 2, 'imagem_mais_cliques': 2, 'q_max': np.float64(0.5549)}\n",
      "{'epsilon': 0.03, 'total_reward': 243, 'imagem_mais_exibida': 1, 'imagem_mais_cliques': 0, 'q_max': np.float64(0.5962)}\n",
      "{'epsilon': 0.04, 'total_reward': 525, 'imagem_mais_exibida': 1, 'imagem_mais_cliques': 1, 'q_max': np.float64(0.6017)}\n",
      "{'epsilon': 0.05, 'total_reward': 489, 'imagem_mais_exibida': 1, 'imagem_mais_cliques': 1, 'q_max': np.float64(0.572)}\n",
      "{'epsilon': 0.1, 'total_reward': 587, 'imagem_mais_exibida': 3, 'imagem_mais_cliques': 3, 'q_max': np.float64(0.6251)}\n",
      "{'epsilon': 0.15, 'total_reward': 584, 'imagem_mais_exibida': 4, 'imagem_mais_cliques': 4, 'q_max': np.float64(0.6342)}\n",
      "{'epsilon': 0.2, 'total_reward': 540, 'imagem_mais_exibida': 4, 'imagem_mais_cliques': 4, 'q_max': np.float64(0.6105)}\n",
      "{'epsilon': 0.5, 'total_reward': 407, 'imagem_mais_exibida': 1, 'imagem_mais_cliques': 1, 'q_max': np.float64(0.6133)}\n",
      "{'epsilon': 1.0, 'total_reward': 228, 'imagem_mais_exibida': 0, 'imagem_mais_cliques': 0, 'q_max': np.float64(0.6204)}\n"
     ]
    }
   ],
   "source": [
    "# List of epsilon values to test\n",
    "epsilons = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.15, 0.2, 0.5, 1.0]\n",
    "\n",
    "# To store the results\n",
    "resultados = []\n",
    "\n",
    "# Loop: for each epsilon in the list...\n",
    "for eps in epsilons:\n",
    "    \n",
    "    # Set the random seed so results are comparable\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Run the algorithm with the current epsilon\n",
    "    bandits, total_reward, q_bandits, num_selected_bandit, acum_reward_bandit = multi_armed_bandit(\n",
    "        num_games=1000, \n",
    "        epsilon=eps,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Save the results as a tuple or dictionary\n",
    "    resultados.append({\n",
    "        'epsilon': eps,\n",
    "        'total_reward': total_reward,\n",
    "        'imagem_mais_exibida': int(np.argmax(num_selected_bandit)),\n",
    "        'imagem_mais_cliques': int(np.argmax(acum_reward_bandit)),\n",
    "        'q_max': round(np.max(q_bandits), 4)\n",
    "    })\n",
    "\n",
    "# Display the results\n",
    "for r in resultados:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4acae63-0794-484b-a0da-432463b9cfea",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "1. If the algorithm does not explore (epsilon = 0), it may randomly fall on the best image and get stuck with it, but this does not make sense in a real situation. Some degree of exploration would be necessary.\n",
    "\n",
    "2. When exploring 100% (epsilon = 1), the algorithm loses some of its functionality by choosing images randomly and missing the opportunity to choose the best images more frequently.\n",
    "\n",
    "3. Low exploration values ​​(epsion = 0.01) may not be enough for the algorithm to evaluate which image is the best. Since 1% of 1,000 is equal to 10, the algorithm had few opportunities to learn the best images.\n",
    "\n",
    "4. Values ​​between 2% and 5% already seem to provide better rewards, since the algorithm had more opportunities to learn which images are the best.\n",
    "\n",
    "5. From about 20% exploration, the algorithm loses the opportunity to focus on the best images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf8fa13-b457-4e04-b875-cca93ffda395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
